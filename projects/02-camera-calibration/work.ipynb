{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKERBOARD = (9, 6)   # dimensions of the _interior_ corners\n",
    "SQUARE_SIZE = 1.0\n",
    "\n",
    "# Containers to hold the points\n",
    "# Object points (3D), like (0,0,0), (1,0,0), (2,0,0), ..., (6,5,0)\n",
    "# These are the corner points in the world coord sys\n",
    "objp = np.zeros((CHECKERBOARD[0] * CHECKERBOARD[1], 3), dtype=np.float32)\n",
    "objp[:, :2] = np.mgrid[0:CHECKERBOARD[0], 0:CHECKERBOARD[1]].T.reshape(-1,2)\n",
    "\n",
    "# Arrays to store object points and image points from all images\n",
    "objpoints = []  # 3D point in real world space\n",
    "imgpoints = []  # 2D points in image plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30 images.\n"
     ]
    }
   ],
   "source": [
    "dirname = \"chessboard\"\n",
    "images = sorted(glob.glob(f\"./data/{dirname}/img_*.jpg\"))\n",
    "\n",
    "print(f\"Found {len(images)} images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to detect chessboard in image './data/chessboard\\img_26.jpg'.\n"
     ]
    }
   ],
   "source": [
    "# Set up termination criteria for corner subpixel refinement\n",
    "criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "\n",
    "for fname in images:\n",
    "    img = cv.imread(fname)\n",
    "    gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Find the chessboard's interior corners\n",
    "    ret, corners = cv.findChessboardCorners(gray, patternSize=CHECKERBOARD, corners=None)\n",
    "\n",
    "    if ret:\n",
    "        # If corners were successfully found\n",
    "        # Add object points\n",
    "        objpoints.append(objp)  # same for every image, since we know the geometry\n",
    "\n",
    "        # Refine the detected image corners, and add them\n",
    "        corners2 = cv.cornerSubPix(\n",
    "            image=gray,\n",
    "            corners=corners,\n",
    "            winSize=(11, 11),\n",
    "            zeroZone=(-1, -1),\n",
    "            criteria=criteria,\n",
    "        )\n",
    "        imgpoints.append(corners2)\n",
    "    else:\n",
    "        print(f\"Failed to detect chessboard in image '{fname}'.\")\n",
    "        objpoints.append(None)\n",
    "        imgpoints.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 20\n",
    "\n",
    "img = cv.imread(images[idx])\n",
    "corners = imgpoints[idx]\n",
    "\n",
    "cv.drawChessboardCorners(\n",
    "    image=img,\n",
    "    patternSize=CHECKERBOARD,\n",
    "    corners=corners,\n",
    "    patternWasFound=True,\n",
    ")\n",
    "cv.imshow('img', cv.cvtColor(img, cv.COLOR_BGR2GRAY))\n",
    "cv.waitKey(3*1_000)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have, for every image in our dataset, a collection of object points in the world coord sys and the corresponding collection of detected points in the image. We can use this entire collection of correspondences to fit our camera model; this is calibration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "objpoints = [objp for objp in objpoints if objp is not None]\n",
    "imgpoints = [imgp for imgp in imgpoints if imgp is not None]\n",
    "\n",
    "ret, mtx, dist, rvecs, tvecs = cv.calibrateCamera(\n",
    "    objectPoints=objpoints,\n",
    "    imagePoints=imgpoints,\n",
    "    imageSize=gray.shape[::-1],\n",
    "    cameraMatrix=None,  # We want to fit this!\n",
    "    distCoeffs=None,    # We want to fit this!\n",
    ")\n",
    "# ret - return code\n",
    "# mtx - camera intrinsics matrix\n",
    "# dist - distortion coefficients\n",
    "# rvecs - (extrinsic) rotation vectors\n",
    "# tvecs - (extrinsic) translation vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1024     0      638   ]\n",
      " [   0      1027    375   ]\n",
      " [   0       0       1    ]]\n",
      "[[ 0.11813414 -0.63363523 -0.00117985 -0.00254147  0.77097999]]\n"
     ]
    }
   ],
   "source": [
    "def print_mtx(mtx: np.ndarray) -> None:\n",
    "    if not mtx.shape == (3,3):\n",
    "        raise ValueError(f\"Camera intrinsics matrix should have shape (3,3); got {mtx.shape}\")\n",
    "    print(f\"[[ {int(mtx[0][0]):^6d}  {int(mtx[0][1]):^6d}  {int(mtx[0][2]):^6d} ]\")\n",
    "    print(f\" [ {int(mtx[1][0]):^6d}  {int(mtx[1][1]):^6d}  {int(mtx[1][2]):^6d} ]\")\n",
    "    print(f\" [ {int(mtx[2][0]):^6d}  {int(mtx[2][1]):^6d}  {int(mtx[2][2]):^6d} ]]\")\n",
    "\n",
    "print_mtx(mtx)\n",
    "\n",
    "print(dist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can I interpret these values?\n",
    "- $f_x = 1024$, $f_y = 1027$ are the focal lengths in pixel units.\n",
    "    - These roughly correspond to horizontal and vertical FOVs as $64.0$ and $38.6$ degrees, respectively, which are plausible.\n",
    "- $c_x = 638$, $c_y = 375$ are the location of the principal point in pixels.\n",
    "    - Note that the center of the image is at $\\left(1280, 720\\right)/2 = \\left(640, 360\\right)$, so this is fairly close."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can we assess the calibration?\n",
    "\n",
    "One method is **re-projection error**, which we calculate by projecting the object points to the image points by iterating the projection model forward; then we compare the results with the actual image points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reprojection error: 0.13\n"
     ]
    }
   ],
   "source": [
    "mean_error = 0.0\n",
    "\n",
    "for i in range(len(objpoints)):\n",
    "    # Use the determined parameters to re-project the object points\n",
    "    imgpoints_proj, _ = cv.projectPoints(\n",
    "        objectPoints=objpoints[i],\n",
    "        rvec=rvecs[i],\n",
    "        tvec=tvecs[i],\n",
    "        cameraMatrix=mtx,\n",
    "        distCoeffs=dist,\n",
    "    )\n",
    "    # Calculate the norm\n",
    "    error = cv.norm(imgpoints[i], imgpoints_proj, cv.NORM_L2) / len(imgpoints_proj)\n",
    "    mean_error += error\n",
    "\n",
    "mean_error /= len(objpoints)\n",
    "\n",
    "print(f\"Mean reprojection error: {mean_error:0.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What about real distances?\n",
    "\n",
    "I noticed that `cv.calibrateCamera` returned extrinsic parameters `(rvecs, tvecs)` for each pose that was used during calibration, but I didn't think I told it anything about my real-world dimensions. It turns out that I _did_,  when I defined the world coordinate system (the planar system on the board).\n",
    "\n",
    "Thus, if I redefine this coordinate system and use the actual physical units, the quantities that are fit by the calibration should be in actual physical units. For example, `tvecs[0]` will be the actual vector from the camera's origin to the chessboard origin at the time of my taking `image[0]`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can we do with this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/chessboard\\\\img_29.jpg'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts = fname.split('.')\n",
    "'.'.join(parts[:-1]) + '_calib.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = images[21]\n",
    "img = cv.imread(fname)\n",
    "h, w = img.shape[:2]\n",
    "newcameramtx, roi = cv.getOptimalNewCameraMatrix(mtx, dist, (w,h), 1, (w,h))\n",
    "\n",
    "# undistort\n",
    "dst = cv.undistort(img, mtx, dist, None, newcameramtx)\n",
    "\n",
    "# crop image\n",
    "x, y, w, h = roi\n",
    "dst = dst[y:y+h, x:x+w]\n",
    "\n",
    "parts = fname.split('.')\n",
    "dstname = '.'.join(parts[:-1]) + '_calib.jpg'\n",
    "cv.imwrite(dstname, dst)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can't we do?\n",
    "\n",
    "In the background of my calibration images, I have a whiteboard on my wall. For a given image, the upper-left corner and the upper-right corner of the whiteboard appear in a pair of pixels in the image. These pixels \"pull back\" to rays in 3D space under the camera model.\n",
    "\n",
    "Unfortunately, this is as far as we can go. I was wondering if I could estimate the length of the whiteboard with this information. But from this single camera's perspective, there is no way to get depth information: we can tell what ray each pixel's actual object lies on, but we don't know about depth.\n",
    "\n",
    "However, if we knew the plane that the whiteboard lies on --- i.e., the wall --- relative to the camera's coordinate system, then we _could_ do this."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the equation $$ \\tilde{x} = K\\left[R\\middle|t\\right]\\tilde{X} $$\n",
    "That maps a point in $\\mathbb{P}^3$ to a point in $\\mathbb{P}^2$ --- that is, a ray in $\\mathbb{R}^3$. Then we \"normalize\" this ray to get the point in pixel coordinates.\n",
    "\n",
    "We can go a little bit in the other direction. Given a pixel $x = \\left(u,v\\right)$ in our image, we can lift it to $\\tilde{x} = \\left(u,v,1\\right) \\in \\mathbb{P}^2$; then apply the intrinsics inverse to get $$K^{-1}\\tilde{x}$$ which is a point in the normalized image plane, in the camera's coordinate system. Equivalently, this is a direction vector $\\vec{d}$ for the ray from the camera center into 3D. The ray itself can be written $$ r\\left(\\lambda\\right) = \\lambda \\left(K^{-1}\\tilde{x}\\right), \\; \\lambda > 0. $$\n",
    "\n",
    "At this point, without knowing the extrinsics $\\left(R,t\\right)$ for the actual object represented by the pixel $\\left(u,v\\right)$, we can go no further in inverting this camera model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, say we happen to know $\\left(R,t\\right)$, which, again, relate the camera's coordinate system to the coordinate system of the wall (the world). Then we know the rays in 3D space --- in the camera's coordinate system --- on which the object points lie. We can\n",
    "1. use intrinsics to pull these rays into the wall's coordinate system; then\n",
    "2. model the wall as a plane in this system, and find where the rays hit the plane."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In detail, recall that extrinsics map from the wall/world coordinate system to camera coordinates as $$ X_c = RX_w + t.$$ We can easily invert this to get $$ X_w = R^T\\left(X_c - t\\right). $$\n",
    "\n",
    "Let a ray's direction vector in the camera coordinate system be $\\vec{r}_c$, and then the ray is $\\lambda \\vec{r}_c$. Then this ray pulls back to $$ \\vec{r}_w = R^T\\left(\\lambda \\vec{r}_c - t\\right) = -R^T t + \\lambda R^T \\vec{r}_c $$ (note: $0\\vec{r}_c$ is the camera center, which pulls back to $\\vec{r}_w = -R^T t$ as expected). Lastly, write the plane of the wall as $$ \\hat{n}^T X_w + d = 0, $$ plug in the parametric form of the ray $$ \\hat{n}^T \\vec{r}_w\\left(\\lambda\\right) + d = 0, $$ and solve for $\\lambda = \\lambda^*$ to find where the ray hits the wall (plane). Then $\\vec{r}_w\\left(\\lambda^*\\right)$ is the object point that we sought (in the wall's coordinate system)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e95c4a395942eb9c87e71123751182b13b020f992d215ee6534c7a9b33e7dda1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
